name: Playlist ETL

on:
  schedule:
    - cron: '30 2 * * *'
  push:
    paths:
      - 'backend/core/management/commands/playlist_etl_modules/**'
      - 'backend/core/management/commands/playlist_etl.py'
  pull_request:
    paths:
      - 'backend/core/management/commands/playlist_etl_modules/**'
      - 'backend/core/management/commands/playlist_etl.py'
  workflow_dispatch:
    inputs:
      clear_gql_cache:
        description: 'Clear GraphQL playlist cache only'
        required: false
        default: false
        type: boolean
      force_refresh:
        description: 'Force refresh by skipping RapidAPI cache and pulling fresh data'
        required: false
        default: false
        type: boolean

jobs:
  run-etl:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.13
        uses: actions/setup-python@v4
        with:
          python-version: 3.13


      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .

      - name: Setup Chrome and ChromeDriver
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
          install-chromedriver: true

      - name: Set PYTHONPATH
        run: echo "PYTHONPATH=$PWD" >> $GITHUB_ENV

      - name: Check runner specs
        run: |
          echo "üîç Runner specifications:"
          echo "CPU cores: $(nproc)"
          echo "Memory: $(free -h | grep Mem | awk '{print $2}')"
          echo "Disk: $(df -h / | tail -1 | awk '{print $4}' | sed 's/G/GB/')"

      - name: Pre-migration database safety check
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          CF_NAMESPACE_ID: ${{ secrets.CF_NAMESPACE_ID }}
          CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        run: make ci-db-safety-check

      - name: Handle existing aggregate table migration
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          CF_NAMESPACE_ID: ${{ secrets.CF_NAMESPACE_ID }}
          CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        run: |
          echo "‚ö° Handling existing aggregate table if needed..."
          cd backend && python manage.py fake_migration_0027

      - name: Apply database migrations to Railway PostgreSQL
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          CF_NAMESPACE_ID: ${{ secrets.CF_NAMESPACE_ID }}
          CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        run: make ci-db-migrate

      - name: Clear GraphQL cache (conditional)
        if: ${{ inputs.clear_gql_cache == true }}
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          CF_NAMESPACE_ID: ${{ secrets.CF_NAMESPACE_ID }}
          CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        run: |
          echo "üóëÔ∏è Clearing GraphQL playlist cache only..."
          cd backend
          python manage.py shell -c "
          from core.utils.local_cache import local_cache_clear, CachePrefix
          playlist_cleared = local_cache_clear(CachePrefix.GQL_PLAYLIST)
          print(f'‚úÖ Cleared {playlist_cleared} playlist cache entries')
          "

      - name: Run Playlist ETL Pipeline
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
          X_RAPIDAPI_KEY_A: ${{ secrets.X_RAPIDAPI_KEY_A }}
          X_RAPIDAPI_KEY_B: ${{ secrets.X_RAPIDAPI_KEY_B }}
          MONGO_URI: ${{ secrets.MONGO_URI }}
          SPOTIFY_CLIENT_ID: ${{ secrets.SPOTIFY_CLIENT_ID }}
          SPOTIFY_CLIENT_SECRET: ${{ secrets.SPOTIFY_CLIENT_SECRET }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          CF_NAMESPACE_ID: ${{ secrets.CF_NAMESPACE_ID }}
          CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        run: |
          if [ "${{ inputs.force_refresh }}" = "true" ]; then
            make run-playlist-etl-force-refresh
          else
            make run-playlist-etl
          fi

      - name: Verify ETL pipeline results
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
        run: |
          cd backend
          echo " ETL Pipeline Results:"
          python manage.py shell -c "
          from core.models import RawPlaylistData, Track, ServiceTrack, PlaylistModel, Genre, Service
          print(f'Raw playlist records: {RawPlaylistData.objects.count()}')
          print(f'Unique tracks: {Track.objects.count()}')
          print(f'Service track entries: {ServiceTrack.objects.count()}')
          print(f'Service playlists: {PlaylistModel.objects.count()}')
          print(f'Genres: {Genre.objects.count()}')
          print(f'Services: {Service.objects.count()}')
          print()
          print(' Breakdown by service:')
          for service in Service.objects.all():
              playlist_count = PlaylistModel.objects.filter(service=service).count()
              track_count = ServiceTrack.objects.filter(service=service).count()
              print(f'{service.name}: {playlist_count} playlists, {track_count} tracks')
          "

      - name: Post-ETL database validation
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
          CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
          CF_NAMESPACE_ID: ${{ secrets.CF_NAMESPACE_ID }}
          CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        run: make ci-db-validate

      - name: Log ETL completion
        run: |
          echo " PostgreSQL ETL Pipeline completed successfully"
          echo "  Data persisted to Railway PostgreSQL database"
          echo " Next ETL run scheduled for tomorrow at 2:30 AM UTC"
